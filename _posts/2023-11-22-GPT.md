回一下**GPT**系列的论文~

下图是顺着时间的这几篇工作的发表，首先是Transformer发表在2017年6月，一年之后GPT诞生了，GPT的核心技术是拿出Transformer的decoder，然后在没有标号的大量文本数据上训练一个语言模型，以获得一个预训练模型，然后再在子任务上做微调，得到每一个任务上所要的分类器。
然后在四个月之后，BERT出来了，BERT是把Transformer的encoder拿过来，然后收集了一个更大的数据集用来做预训练，实现了比GPT好很多的效果。BERT里面包含BERT-base和BERT-large，BERT-base的模型跟GPT大小一致，BERT-large更大一些，这两个效果都强于GPT。
再在四个月之后，GPT-2出现了，他们收集了一个更大的数据集，训练了一个更大的模型，超过了BERT-large的模型大小，发现这个很适合做zero-shot，但效果上没有那么惊艳。于是在一年多之后，GPT-3推出，GPT-3相对于GPT-2的改进，就是数据和模型都增大了一百倍，**大力出奇迹！**
效果非常惊艳，但GPT-3太大了导致其他团队很难复现结果。（Transformer和BERT都是Google的一些独立团队做的，GPT系列都是OpenAI推出）

~~~mermaid
graph LR
  Transformer_06/2017 --> GPT_06/2018 --> BERT_10/2018 --> GPT-2_02/2019 --> GPT-3_05/2020
~~~

