# 回顾一下**GPT**系列的论文~ 

下图是顺着时间的这几篇工作的发表，首先是Transformer发表在2017年6月，一年之后GPT诞生了，GPT的核心技术是拿出Transformer的decoder，然后在没有标号的大量文本数据上训练一个语言模型，以获得一个预训练模型，然后再在子任务上做微调，得到每一个任务上所要的分类器。
然后在四个月之后，BERT出来了，BERT是把Transformer的encoder拿过来，然后收集了一个更大的数据集用来做预训练，实现了比GPT好很多的效果。BERT里面包含BERT-base和BERT-large，BERT-base的模型跟GPT大小一致，BERT-large更大一些，这两个效果都强于GPT。
再在四个月之后，GPT-2出现了，他们收集了一个更大的数据集，训练了一个更大的模型，超过了BERT-large的模型大小，发现这个很适合做zero-shot，但效果上没有那么惊艳。于是在一年多之后，GPT-3推出，GPT-3相对于GPT-2的改进，就是数据和模型都增大了一百倍，**大力出奇迹！**
效果非常惊艳，但GPT-3太大了导致其他团队很难复现结果。（Transformer和BERT都是Google的一些独立团队做的，GPT系列都是OpenAI推出）


 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/eb969152-8901-4dcb-acb7-4a9861cfb479)

> ## Improving Language Understanding by Generative Pre-Training (GPT)

GPT使用没有标号的文本、先训练好预训练模型再来微调。使用没有标号的文本时遇到了一些困难，第一点是不知道用什么样的优化函数，第二点是怎么把学到的表示一致地迁移到所有子任务上。
GPT基于半监督方法（也称作自监督）使用Transformer框架。

> #### GPT分为两个步骤
>
> - 1.在没有标号的数据上做预训练
> - 2.有监督的微调

> ## Language Models are UNsupervised Multitask Learners (GPT-2)

GPT-2训练了一个15亿参数的模型，BERT-large最大也只是3.4亿。但模型效果并没有好过BERT太多，因此作者找到了另一个观点，即**zero-shot**

GPT-2仍然是在做大语言模型，但在做下游任务时，不再使用标号数据做有监督的微调也不需要再训练一个模型，而是去做一个zero-shot的设定。
GPT-2尝试了从一个新的角度去看待问题。

相比于GPT，GPT-2使用了prompt去做zero-shot。此外，作者基于Reddit收集了一个800万个文本的40GB数据集，并构建了四个模型。

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/54da798c-4067-4191-8fe7-2ec24a226423)

 > ## Language Models are Few-Shot Learners (GPT-3)

GPT-3具有1750亿个参数。GPT-3在做下游子任务时，不做任何的梯度更新或微调，GPT-3是给一些少量的样本，即few-shot，不做更新，取得了NLP所有任务都很强大的效果。GPT-3整个模型是跟GPT-2一样的，然后在上面做了一些改进，使用了Sparse Transformer的架构。

meta learning & in-context learning
 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/e66f28c6-8e34-43dd-959c-461bfa945b21)

> #### 评估GPT-3有三个设定
>
> - 1.few-shot learning（10-100个训练样本）
> - 2.one-shot learning
> - 3.zero-shot learning

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/70c4746e-dbd3-41d5-a694-db60f6ffda27)
 
 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/12c0b978-ad22-4922-8463-ad81da21ff56)

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/e26477d3-b2bb-42a0-a278-b61ee8fdf45b)

GPT-3的训练数据是基于common Crawl dataset，进行了过滤、logistics对抗样本分类和去重操作。

> #### GPT-3的局限性：
>
> - 1.长文本还比较困难；
> - 2.结构和算法的局限性；
> - 3.对每一个词都均匀学习，不会划重点；
> - 4.样本有效性不够；
> - 5.泛化性；
> - 6.训练成本高；
> - 7.可解释性，不知道做关键决策的是哪些权重。





