**CLIP**是OpenAI在2021年1月份发布的一个多模态模型，同时还发布了另一个模型是DALL-E。但**CLIP**和DALL-E有本质的区别，**CLIP**是是用文本作为监督信号来训练可迁移的视觉模型，DALL-E是基于文本来生成图像的模型。
**CLIP**的英文全称是Contrastive Language-Image Pre-training，即一种基于对比文本-图像对的预训练方法或者模型。本质上，它是一种基于对比学习的多模态模型，去学习文本-图像对的匹配关系。
如下图所示，CLIP包括两个模型：Text Encoder和Image Encoder，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。
 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/de3e280e-f72f-42f8-ac0b-fb8099fb1be9)


