**CLIP**是OpenAI在2021年1月份发布的一个多模态模型，同时还发布了另一个模型是DALL-E。但**CLIP**和DALL-E有本质的区别，**CLIP**是是用文本作为监督信号来训练可迁移的视觉模型，DALL-E是基于文本来生成图像的模型。

**CLIP**的英文全称是Contrastive Language-Image Pre-training，即一种基于对比文本-图像对的预训练方法或者模型。本质上，它是一种基于对比学习的多模态模型，去学习文本-图像对的匹配关系。
> 1. Contrastive pre-training

如下图所示，CLIP包括两个模型：Text Encoder和Image Encoder，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/de3e280e-f72f-42f8-ac0b-fb8099fb1be9)

这里对提取的文本特征和图像特征进行对比学习。对于一个包含```N```个文本-图像对的训练batch，将```N```
个文本特征和```N```个图像特征两两组合，CLIP模型会预测出
```N^2```个可能的文本-图像对的相似度，计算文本特征和图像特征的cosine similarity，即上图所示的矩阵。这里共有```N```
个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的```N^2-N```
个文本-图像对为负样本，那么CLIP的训练目标就是最大化```N```个正样本的相似度，同时最小化```N^2-N```个负样本的相似度。

**CLIP**还有一个很夸张之处在于从互联网收集了共4个亿的文本-图像对的数据集**WebImageText**。
按照文本的单词量，它和训练GPT-2的WebText规模类似，如果从数量上对比的话，它比谷歌的JFT-300M数据集多一个亿，所以说这是一个很大规模的数据集。

**CLIP**是一个多模态模型，主要用来训练可迁移的视觉模型。
在其论文中，它的Text Encoder使用了Text Transformer模型，该模型包含63M个参数。此外，Image Encoder使用了两种不同的架构。一种是常用的CNN架构ResNet，另一种是基于Transformer的ViT。

在ResNet架构中，有5种不同大小的模型被使用：ResNet50，ResNet101，RN50x4，RN50x16和RN50x64（后面三个模型是根据EfficientNet的缩放规则对ResNet50分别增大4x，16x和64x得到）。而在ViT架构中，有3种不同大小的模型被使用：ViT-B/32，ViT-B/16和ViT-L/14。

所有模型都进行了32个epoch的训练，使用AdamW优化器，而且训练过程采用了一个较大的批量大小：32768。由于数据量较大，最大的RN50x64需要在592个V100卡上训练18天，而最大的ViT模型ViT-L/14需要在256张V100卡上训练12天，这表明训练**CLIP**需要消耗大量资源，不是一般人玩得动的。

对于ViT-L/14，还在336的分辨率下额外finetune了一个epoch来增强性能，论文发现这个模型效果最好，将其记为ViT-L/14@336，论文中进行对比实验的**CLIP**模型也采用了这个模型。

以上就是**CLIP**的原理。与CV中常用的先预训练然后微调不同，**CLIP**可以直接实现zero-shot的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，这也是**CLIP**的亮点，这里我理解的还是因为样本足够大，模型记住了足够多的数据。

用**CLIP**实现zero-shot分类很简单，只需要简单的两步：

> 2. Create dataset classifier from label text

根据任务的分类标签构建每个类别的描述文本：A photo of {label}，然后将这些文本送入Text Encoder得到对应的文本特征，如果类别数目为
```N```，那么将得到个```N```文本特征。

> 3. Use for zero-shot prediction

将要预测的图像送入Image Encoder得到图像特征，然后与```N```个文本特征计算缩放的余弦相似度（和训练过程一致），然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率。

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/5fac6d7e-be70-4a90-9417-8ca061074168)

图中可以看到，利用CLIP的多模态特性为具体的任务构建了动态的分类器，其中Text Encoder提取的文本特征可以看成分类器的weights，而Image Encoder提取的图像特征是分类器的输入。

接下来，论文给出了一些对比实验结果。

首先是CLIP和17年的一篇工作Learning Visual N-Grams from Web Data的在3个分类数据集上zero-shot效果对比，如下表所示，可以看到CLIP模型在效果上远远超过之前的模型，其中在ImageNet数据集可以达到76.2，这和全监督的ResNet50效果相当，不用任何训练数据就能达到这个效果是相当惊艳的。

 ![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/02984edb-3e2f-44b7-926c-fafd9ac56d70)

其实，使用CLIP进行zero-shot分类，另外一个比较重要的地方是文本描述的生成，上面的是采用A photo of {label}，但其实也有其它选择，比如我们直接用类别标签，这其实属于最近NLP领域比较火的一个研究：prompt learning或者prompt engineering，简单来说，prompt learning的核心是通过构建合适的prompt（提示）来使预训练模型能够直接应用到下游任务，这和之前的预训练+微调属于不同的范式。论文也说了，如果我们直接采用类别标签作为文本描述，那么很多文本就是一个单词，存在多义性，缺少具体的上下文，而且也和CLIP的训练数据不太一致，效果上会不如采用A photo of {label}。

作者表示论文的实验了采用80个不同的prompt来进行集成，发现在ImageNet数据集上能带来3.5%的提升，具体见CLIP公开的notebook。下图对比了基于ResNet的CLIP模型直接采用类别名与进行prompt engineering和ensembling的效果对比：

<img width="259" alt="image" src="https://github.com/zw510644628/zw510644628.github.io/assets/50043212/eb184d1c-c358-49a1-9639-efcc19e9f5da">

更进一步地，论文还对比了Zero-shot CLIP和ResNet50 linear probing（ImageNet数据上预训练，在加上线性分类层进行finetune）在27个数据集上表现，如下图所示，其中在16个数据集上CLIP可以超过ResNet50（对物体分类比较敏感）。但是在一些特别的，复杂的或者抽象的数据集（如纹理分类）上CLIP表现较差，比如卫星图像分类，淋巴结转移检测，在合成场景中计数等，CLIP的效果不如全监督的ResNet50，作者认为可能需要做一些few-shot。如果认真看下图的话，CLIP表现较差的竟然还有MNIST数据集，分类准确度只有88%，这是不可思议的，因为这个任务太简单了，通过对CLIP训练数据进行分析，作者发现4亿的训练数据中基本上没有和MNIST比较相似的数据，所以这对CLIP来说就属于域外数据了，表现较差就比较容易理解了。这也表明：**CLIP依然无法解决域外泛化这个深度学习难题。**

<img width="239" alt="image" src="https://github.com/zw510644628/zw510644628.github.io/assets/50043212/c1772a86-8a17-4c9a-b216-4c08b07e9fe5">

除了zero-shot对比，论文还对比few-shot性能，即只用少量的样本来微调模型，这里对比了3个模型：在ImageNet21K上训练的BiT-M ResNet-152x2，基于SimCLRv2训练的ResNet50，以及有监督训练的ResNet50。可以看到CLIP的zero-shot和最好的模型（BiT-M）在16-shot下的性能打平。另外一个比较有意思的结果是：虽然CLIP在few-shot实验中随着样本量增加性能有提升，但是1-shot和2-shot性能比zero-shot还差，这个作者认为主要是CLIP的训练是用文本去引导做多模态的学习，比常规的有监督训练更加强大。
最后一个观察，就是随着训练样本的增多，few-shot CLIP的模型效果是最好的，超越了之前的所有方法以及zero-shot CLIP。

<img width="236" alt="image" src="https://github.com/zw510644628/zw510644628.github.io/assets/50043212/7dd47b79-9a9f-4403-9098-e7f0edc60d07">

接下来，看下如果下游任务用全部的数据集，CLIP的效果会如何。因此，论文进行了representation Learning实验，即自监督学习中常用的linear probe和finetune。在这里，作者就用linear probe而不用finetune，finetune涉及要调的超参数和涉及方案太多了。linear probe就是用训练好的模型先提取特征，然后用一个线性分类器来有监督训练。下图为不同模型在27个数据集上的average linear probe score对比，可以看到CLIP模型在性能上超过其它模型，而且计算更高效：

![image](https://github.com/zw510644628/zw510644628.github.io/assets/50043212/6b607821-237f-4faa-b8af-fc6a1937464a)
